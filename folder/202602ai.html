<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ice Hockey ML Analysis - Quta's Portfolio</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <style>
        :root {
            --bg-color: #e0f2f7;
            --main-navy: #1a3a5f;
            --accent-blue: #4682b4;
            --white: #ffffff;
            --code-bg: #0d1117;
            --comparison-gray: #f8f9fa;
        }

        body {
            font-family: "Helvetica Neue", Arial, "Hiragino Kaku Gothic ProN", sans-serif;
            background-color: var(--bg-color);
            max-width: 1000px;
            margin: 0 auto;
            padding: 40px 20px;
            color: #333;
            line-height: 1.6;
        }

        header { text-align: center; margin-bottom: 40px; }
        
        nav ul { list-style: none; padding: 0; display: flex; justify-content: center; gap: 10px; flex-wrap: wrap; }
        nav ul li a {
            text-decoration: none; background-color: var(--main-navy); color: white; padding: 10px 20px; border-radius: 30px; font-weight: bold; transition: 0.3s;
        }
        nav ul li a:hover { opacity: 0.8; }

        h2 {
            border-left: 6px solid var(--main-navy);
            padding: 5px 15px;
            color: var(--main-navy);
            background: rgba(255,255,255,0.5);
            margin: 40px 0 20px 0;
        }

        h3 { color: var(--main-navy); margin-top: 30px; border-bottom: 1px solid var(--accent-blue); display: inline-block; }

        .description {
            background: white;
            padding: 25px;
            border-radius: 12px;
            margin-bottom: 30px;
            font-size: 0.95rem;
            box-shadow: 0 4px 15px rgba(0,0,0,0.05);
        }

        /* 比較テーブルのスタイル */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 8px;
            overflow: hidden;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid #ddd;
            padding: 15px;
            text-align: left;
        }
        .comparison-table th { background-color: var(--main-navy); color: white; }
        .comparison-table tr:nth-child(even) { background-color: #f2f2f2; }

        /* 結果画像のレイアウト */
        .result-container {
            display: flex;
            gap: 20px;
            margin: 30px 0;
            flex-wrap: wrap;
        }
        .result-card {
            flex: 1;
            min-width: 300px;
            background: white;
            padding: 15px;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            text-align: center;
        }
        .result-card img {
            width: 100%;
            border-radius: 8px;
            margin-bottom: 10px;
        }
        .result-card p { font-size: 0.85rem; color: #666; font-weight: bold; }

        .highlight { font-weight: bold; color: var(--main-navy); }

        .back-link {
            display: inline-block;
            margin-top: 50px;
            color: var(--main-navy);
            text-decoration: none;
            font-weight: bold;
            padding: 10px 20px;
            border: 2px solid var(--main-navy);
            border-radius: 30px;
            transition: 0.3s;
        }
        .back-link:hover { background: var(--main-navy); color: white; }

        hr { border: 0; border-top: 1px solid #ccc; margin: 40px 0; }
    </style>
</head>
<body>

    <header>
        <h1>Source Code & Analysis</h1>
        <nav>
            <ul>
                <li><a href="../index.html">ホーム</a></li>
                <li><a href="../works.html">成果</a></li>
                <li><a href="../readinglog.html">読書履歴</a></li>
                <li><a href="../hobby.html">趣味</a></li>
                <li><a href="../link.html">リンク</a></li>
            </ul>
        </nav>
    </header>

    <div class="section">
        <h2>アイスホッケーゲームにおける強化学習と反実仮想機械学習の比較</h2>
        
          <div class="description">
              <p>ネット記事を漁っていると<strong>反実仮想機械学習</strong>というものが存在していると知りました。私は塾講師のアルバイトで古典も担当していて、文法に「反実仮想」の表現があるため、非常に興味が湧きました。</p>
              <p>そこで、なじみ深い強化学習(DQN)の復習も兼ねて、それぞれの性能比較を行いました。題材として選んだのは「アイスホッケーゲーム」です。球の位置を追って返球するBotに対し、機械学習モデルを戦わせることで、その成長過程や性能差を直感的に可視化することを目指しました。</p>
          </div>

        <hr>

        <h3>1. 反実仮想機械学習とは何か</h3>
        <div class="description">
          <p>
            古典には、「もし～であれば、・・・だったのに」という反実仮想の意味を持つ助動詞「まし」が存在します。<br>
            用例としては、「世の中に絶えて桜のなかりせば春の心はのどけから<strong>まし</strong>(在原業平)」(訳:もし世の中に桜が全くなかったら、春の人々の心は穏やかだろうに)　が有名で、「桜が美しいせいで、花見等で春は落ち着いていられない」という風流な心が感じられます。
          </p>
          <p>
            反実仮想機械学習は、この反実仮想の考えを機械学習に応用したような手法で、具体的には<strong>「もし別の行動をとっていたら、結果はどう変わっていたか？」</strong>という過去の選択に対するシミュレーションを行い、未来の学習に役立てます。<br>
            実体験に加えて仮想的な試行データを生成することで、学習効率を向上させ、医療やロボティクスなど様々な分野で活用されています。</p>
          </p>
        </div>

        <h3>2. 強化学習との違い</h3>
        <table class="comparison-table">
            <thead>
                <tr>
                    <th>項目</th>
                    <th>標準的な強化学習 (DQN)</th>
                    <th>反実仮想機械学習 (Deep Causal RL)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>学習の基盤</strong></td>
                    <td>試行錯誤による報酬の最大化</td>
                    <td>環境の因果モデル + 試行錯誤</td>
                </tr>
                <tr>
                    <td><strong>データ活用</strong></td>
                    <td>実際に起きた事象のみを学習</td>
                    <td>「もし〜していたら」という仮想データも活用</td>
                </tr>
                <tr>
                    <td><strong>報酬設計</strong></td>
                    <td>詳細な設計が不可欠</td>
                    <td>モデルが因果を補完し、比較的シンプルで済む</td>
                </tr>
            </tbody>
        </table>

        <h3>3. ゲーム・学習モデル</h3>
        <div class="description">
            <p>今回の実装では、以下の2つのアプローチでエージェントを構成しました。</p>
            <ul>
                <li><strong>反実仮想モデル:</strong> 内部に環境の挙動を予測する <span class="highlight">WorldModel</span> を構築。学習が進むと「成功確率の高い擬似データ」を自ら生成し、経験を補強します。</li>
                <li><strong>DQNモデル:</strong> 報酬設計を極めて緻密に実施。ガイドとなる報酬を細かく設定し、行動の指針を直接与えます。</li>
            </ul>

            <h4>アイスホッケーゲームの基本仕様</h4>
            <p>学習の土台となるゲーム環境は、以下のルールで構築しています。</p>
            <ul>
                <li><strong>画面構成:</strong> $500 \times 700$ ピクセルのフィールド。中央のラインを境に、上がAI、下がBot(または人間)の陣地となります。</li>
                <li><strong>得点システム:</strong> ボールが画面下端(Bot側)を超えればAIの得点、上端(AI側)を超えればBotの得点となります。</li>
                <li><strong>サーブ権:</strong> 直前のラリーで得点した側のパドル位置から、ボールがリセットされ次のゲームが開始されます。</li>
                <li><strong>難易度調整:</strong> $50$ エピソードごとに直近の勝率を確認し、以下の条件で環境を動的に変化させます。
                  <ul>
                      <li><strong>強化(しきい値 $70\% 以上):</strong> Botの移動速度を $+0.5$、パワーを $+0.2$ 増加させ、負荷を高めます。</li>
                      <li><strong>緩和(しきい値 $20\% 未満):</strong> Botの移動速度を $-0.5$、パワーを $-0.2$ 減少させ、学習の停滞を防ぎます。</li>
                      <li><strong>下限設定:</strong> Botの速度が $2.0$ を下回ることはなく、最低限のゲーム性を維持した状態で再学習を促します。</li>
                  </ul>
                </li>
            </ul>

            <h4>詳細な報酬設計(DQNモデルにおける Shaping Rewards)</h4>
            <p>AIが効率的に「打ち返し」と「勝利」を学習できるよう、以下の報酬関数を定義しました。</p>
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>カテゴリ</th>
                        <th>条件</th>
                        <th>報酬額</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>距離報酬</strong></td>
                        <td>パドルがボールの水平位置に近づく</td>
                        <td>$+0.05$</td>
                    </tr>
                    <tr>
                        <td><strong>範囲報酬</strong></td>
                        <td>ボールがパドルの横幅($80$px)の範囲内にある</td>
                        <td>$+0.1$</td>
                    </tr>
                    <tr>
                        <td><strong>イベント報酬</strong></td>
                        <td>ボールを打ち返す</td>
                        <td>$+1.0$</td>
                    </tr>
                    <tr>
                        <td><strong>スキルボーナス</strong></td>
                        <td>高速移動(Smash)状態で打ち返す</td>
                        <td>$+0.5$</td>
                    </tr>
                    <tr>
                        <td><strong>終端報酬(勝)</strong></td>
                        <td>相手ゴールにボールを入れる</td>
                        <td>$+5.0$</td>
                    </tr>
                    <tr>
                        <td><strong>終端報酬(負)</strong></td>
                        <td>自陣にボールを入れられる</td>
                        <td>$-1.0$</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>4. 学習結果の比較</h3>
        <div class="result-container">
            <div class="result-card">
                <img src="icehockey_hanjitu.png" alt="反実仮想モデルの学習結果">
                
            </div>
            <div class="result-card">
                <img src="icehockey_kyouka.png" alt="強化学習モデルの学習結果">
            </div>
            <div class="video-wrapper">
              <video>
                  controls 
                  preload="metadata" 
                  style="width: 100%; height: 100%;">
                  <source src="hanjitukaso.mp4" type="video/mp4">
                  <p>お使いのブラウザはビデオタグをサポートしていません。動画は <a href="videos/icehockey_demo.mp4">こちらからダウンロード</a> できます。</p>
              </video> 
            </div>
            <div class="result-card">
              <p>【反実仮想】最大BotSpeed:13.0 , 1万回試行時BotSpeed:12.5 , 2万試行時BotSpeed:12.5 , 2万回までの時間:1h25m</p>
              <p>【強化学習】最大BotSpeed:13.0 , 1万回試行時BotSpeed:12.5 , 2万試行時BotSpeed:12.5 , 2万回までの時間:1h25m</p>
              <p>よって、反実仮想機械学習は、強化学習に比べて、であると言える。</p>
            </div>
        </div>

        <h3>5. 考察</h3>
        <div class="description">
            <p>上記の結果は、過去のデータをうまく活用したことによる性能向上と言える。<br>
              ただし、報酬条件の微小な不均衡や、過去の結果の考慮に入れるパラメータ数（移動距離のみ考慮）によっては、強化学習の方が高性能である結果が確認された。<br>
              故に、反実仮想機械学習モデルの方が優れているわけではなく、<strong>緻密な報酬設計を施したDQN</strong>も場面に応じて有効であると言えます。<br>
              パラメータを多く上げることで計算量が増え計算時間がかかる反実仮想機械学習モデルか、計算時間は比較的短いがパラメータ次第で精度が大きく変動する強化学習モデルかといったトレードオフがあると感じました。<br>
              （報酬条件を細かく列挙するのであれば、もはや強化学習の良さは半減しているように感じるが、、、）<br>
              つまり、反実仮想機械学習モデルが有効に機能するためには、モデルが環境の因果を捉えるのに十分な表現力(パラメータ数)等が重要であると考えられます。
            </p>
        </div>

        <h3>6. まとめ</h3>
        <div class="description">
            <p>今回の比較を通じ、反実仮想機械学習は<strong>「実データを得るコストが高い環境」</strong>や、<b>「ルールは明確だが状況の変化が激しい環境」</b>において、単なる試行錯誤以上の力を発揮することが分かりました。<br>
              塾講師として教えている古典の「反実仮想」が、最新のAI技術の中で「効率的な成長」の鍵となっている事実に感動し、今後、反実仮想機械学習モデルが目に見える形で活躍していく日が楽しみになりました。
            </p>
        </div>
        
        <h3>7. おまけ</h3>
        <div class="description">
            <p>反実仮想機械学習モデルの学習済みモデルと私が対戦したところ、10戦1勝9負で、<strong>コテン</strong>パにやられました。</p>
            <div class="video-wrapper">
              <video>
                  controls 
                  preload="metadata" 
                  style="width: 100%; height: 100%;">
                  <source src="battle.mp4" type="video/mp4">
                  <p>お使いのブラウザはビデオタグをサポートしていません。動画は <a href="videos/icehockey_demo.mp4">こちらからダウンロード</a> できます。</p>
              </video> 
            </div>
        </div>

        <a href="../works.html" class="back-link">← 成果一覧へ戻る</a>
    </div>

</body>
<style>
    .video-wrapper {
        position: relative;
        padding-bottom: 177.8%;
        height: 0;
        overflow: hidden;
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.15);
        background: #000;
        margin: 20px 0;
    }
    .video-wrapper video {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        object-fit: contain;
        background-color: #000;
    }
</style>
</html>
